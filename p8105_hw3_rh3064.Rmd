---
title: "p8105_hw3_rh3064"
author: "Rahul Hosalli"
date: '`r Sys.Date()`'
output: github_document
---

```{r include=FALSE}
library(tidyverse)
library(viridis)
library(knitr)
library(patchwork)
```

# Problem 2

## Data Loading

```{r}
accel_data <- read_csv("./Data/accel_data.csv")
```

## Data Tidying/Wrangling

The following chunk of code cleans the column names with `janitor::clean_names()`, then using `mutate()` creates a new variable *day_type* which indicates if it was a weekend or weekday. *day* and *day_type* are coerced into factors.

Finally `relocate()` is used to reorder the tibble so that more useful columns are first (*day_id*, *week*, *day*, *day_type*).

`head()` is used to take a glance at the tibble and check the variable classes, which seem reasonable.

Finally a new dataset is created by pivoting the original data with `pivot_longer()` so that the data is tidy.

```{r}
accel_data <- 
  accel_data %>%
  janitor::clean_names() %>%
  mutate(
    
    day_type = case_when(day == "Saturday" | day == "Sunday" ~ "Weekend",
                         TRUE ~ "Weekday"),
    
    day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday",
                                 "Friday", "Saturday", "Sunday")),
    day_type = factor(day_type)
    
    ) %>%
  relocate(day_id, week, day, day_type)

head(accel_data)

accel_long <- accel_data %>% 
  pivot_longer(activity_1:activity_1440, 
               names_to = "minute", 
               names_prefix = "activity_", 
               values_to = "activity_count") %>%
  mutate(minute = as.numeric(minute))

```

## Data Description

There are `r nrow(accel_long)` rows and `r ncol(accel_long)` columns in the *`accel_long`* tibble. *day_id* corresponds to the actual study data of collection. In the original `accel_data` tibble this was of length 35, and corresponded to the unique day of collection. Following the pivot, this variable was duplicated numerous times, as the length of the pivoted tibble is determine by 1440 minutes \* 35 days. The numeric variable *week* corresponds to the study week, and *day* corresponds to the day of week. The day variable was coerced to a factor and assigned ordered levels to ensure proper formatting later. *day_type* is a factor variable corresponding to the type of day (weekend vs weekday) and is unordered. *minute* is a numeric variable corresponding to the minute of the day (from 1 to 1440 for each unique day). Finally, *activity_count* is numeric variable of accelerometer data per minute.

## Total Activity per Day

Total activity count per day per week is calculated by first grouping the data using `group_by()` . Next, `summarise()` is used with `sum()` to calculate the activity totals. This is then piped to `pivot_wider()` to produce a more readable, non-tidy dataframe which is finally outputted via `knitr::kable()`.

```{r}
accel_long %>%
  group_by(week, day) %>%
  summarise(activity_total = sum(activity_count)) %>%
  pivot_wider(names_from = "day",
              values_from = "activity_total") %>%
knitr::kable(caption = "Total Activity Counts by Week and Day")
```

There is no obvious trend in activity count data, although there is a very low count on Saturday in week 4 and 5; this count corresponds to an activity count of 1 per minute, while the average activity count per minute is `r round(mean(accel_long$activity_count), 2)`. This might indicate an error with the accelerometer on those days, or non-use of the device.

## Plots

```{r}
ggplot(accel_long, mapping = aes(minute, activity_count)) +
  geom_line(aes(color = day), alpha =0.7) +
  labs(
    x = "Minute of the Day", 
    y = "Activity Count", 
    title = "Activity Counts per Minute") +
  
  scale_color_viridis(
    name = "Day of the Week",
    discrete = TRUE
  )
```

The largest spike in counts is around the 1250 minute mark, which corresponds to around around 8:30 to 9:00 PM. The lowest activity is seen in the first 300 or so minutes, which corresponds to before 5AM, i.e. when they were asleep. General trends by day are difficult to parse due to the dense overlap of lines.

```{r}
ggplot(accel_long, mapping = aes(minute, activity_count)) +
  geom_smooth(aes(color = day)) +
  labs(
    x = "Minute of the Day", 
    y = "Activity Count",
    title = "Activity Counts per Minute") +
  
  scale_color_viridis(
    name = "Day of the Week",
    discrete = TRUE
  )
```

If we use `geom_smooth()` we can get a better sense of activity count trends by day of the week. There is a spike of activity on Sunday at around minute 600, which corresponds to 10AM.

# Problem 3

## Data Loading

```{r}
library(p8105.datasets)
data("ny_noaa")
```

## Data Cleaning

Initial the data is cleaned by coercing tmax and tmin into integers. tmax tmin and prcp are then divided by 10 to get the correct C and mm values (originally the data is provided in tenths of degrees C or tenths of mm).

Then, `janitor::clean_names()` is used, and *date* is separate into three columns with `separate()`. The *year, month* and *date* variables are coerced into integer variables. *tmax* and *tmin* are coerced into integers as well. Alongside *prcp*, these three variables are provided as tenths if degrees C or tenths of mm, so they are divided by 10 to get degrees C or mm measurements.

```{r}
noaa_df <- ny_noaa %>%
  janitor::clean_names() %>%
  
  separate(date, into = c("year", "month", "date"), sep = "-") %>% 
  
  mutate(year = as.integer(year),
         month = as.integer(month),
         date = as.integer(date),
         
         tmax = as.integer(tmax)/10,
         tmin = as.integer(tmin)/10,
         prcp = prcp/10)
         
head(noaa_df)

noaa_df %>% 
  summarise(n_miss = sum(is.na(.)),
            n_obs = n()*ncol(.),
            percent_miss = (n_miss/n_obs)*100) %>% 
  knitr::kable()
```

## Data Description

The data contains `r nrow(noaa_df)` rows and `r ncol(noaa_df)` columns. *id* indicates the weather station ID, why *year, month* and *day* indicate the date of observation. *prcp* is the precipitation in mm, while *snow* is snowfall in mm and *snwd* is the snow depth in mm. *tmax* and *tmin* are the maximum and minimum temperature on a given date, in degrees C. Roughly 14.5% of the total cells of the dataframe are missing. The most commonly observed snowfall value (ignoring NA's) is `r DescTools::Mode(noaa_df$snow, TRUE)` which is likely due to the fact that snowfall is expected to occur during the winter months, and wouldn't occur for most of the year.

## Plots

### Plot 1

```{r}
plot1_df<- noaa_df %>%
  mutate(month_name = month.name[month]) %>%
  filter(
    month_name %in% c(month.name[1], month.name[7])) %>% 
  group_by(id, year, month_name) %>% 
  summarize(
    tmax_avg = mean(tmax)
  )

ggplot(plot1_df, mapping = aes(x = year, y = tmax_avg, color = id)) +
  geom_line(alpha = 0.5) +
  facet_grid(. ~ month_name) +
  labs(
    x = "Year",
    y = "Mean Maximum Temperature (C)",
    title = "Mean max temperature 1981-2010 : January vs. July"
  )+
  theme(legend.position = "none") +
  scale_color_viridis(option = "inferno", discrete = TRUE)
  
```

The January average temperature is roughly between -10C and 10C, while the July average temperature is between 20C and 35C. Mean maximum temperature seems to go up and down year to year in both January and July. There were outlier months around 1994 an 2004 in January, and in 1987 in July at a single weather station.

### Plot 2

```{r}
plot1 <- noaa_df %>% 
  ggplot(mapping = aes(x = tmin, y = tmax)) +
  geom_hex(bins = 100) +
  labs(
    x = "Minimum Temperature (C)",
    y = "Maximum Temperature (C)",
    title = "Hexagonal Heatmap of Minimum and Maximum Temperature"
  )
plot2 <- noaa_df %>% 
  filter(snow > 0 & snow < 100) %>%
  ggplot(mapping = aes(x = snow, color = year, group = year)) +
  geom_density() +
  scale_color_viridis() +
  labs(
    x = "Snowfall (mm)",
    y = "Density",
    title = "Distribution of Snowfall by Year"
  )

plot1/plot2
  
```

The hexagonal heat map shows that minimum and maximum temperature seem to have a positive, monotonic relationship.

The snowfall distributions plot shows that year to year differences in snowfall distributions is small.
